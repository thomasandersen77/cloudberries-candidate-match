server:
  forward-headers-strategy: framework

spring:
  servlet:
    multipart:
      enabled: true
      file-size-threshold: 128KB
      max-file-size: 10MB
      max-request-size: 10MB
      resolve-lazily: true
      location: /tmp/uploads
  datasource:
    url: jdbc:postgresql://localhost:5433/candidatematch?sslmode=disable
    username: candidatematch
    password: candidatematch123
    hikari:
      maximum-pool-size: 10
      minimum-idle: 5
      connection-timeout: 10000
      idle-timeout: 300000
      max-lifetime: 1200000
      validation-timeout: 5000
      schema: public
  liquibase:
    change-log: classpath:/db/changelog/db.changelog-master.xml
    enabled: true
    contexts: "default,pgvector"
  mvc:
    servlet:
      path: /api
# application-prod.yml

# Make external services optional
flowcase:
  apiKey: ${FLOWCASE_API_KEY}
  baseUrl: ${FLOWCASE_BASE_URL}

openai:
  apiKey: ${OPENAI_API_KEY}
  model: gpt-5
  assistantId: ${OPENAI_ASSISTANT_ID}

gemini:
  apiKey: ${GEMINI_API_KEY}
  
  # Model for project analysis (PDF upload, Må/Bør-krav extraction)
  # gemini-3-pro-preview gives best quality but has capacity limits
  # For PDF analysis, this model is used
  model: gemini-3-pro-preview
  
  # Model for candidate ranking (batch operations via Files API)
  # PRIMARY: gemini-2.5-pro - stable, 2M context, high rate limits
  # This is used as the first choice for batch ranking
  matchingModel: gemini-2.5-pro
  
  # FALLBACK: gemini-2.5-flash - fast, used if matchingModel returns 503
  flashModel: gemini-2.5-flash
  
  fileStoreName: candidate-cv-store
  useFilesApi: true  # MUST be true for Files API batch matching

anthropic:
  apiKey: ${ANTHROPIC_API_KEY}
  model: ${ANTHROPIC_MODEL:claude-3-5-sonnet-20241022}
  baseUrl: ${ANTHROPIC_BASE_URL:https://api.anthropic.com}

embedding:
  enabled: true
  provider: GEMINI
  model: text-embedding-004
  dimension: 768

ai:
  ollama:
    base-url: http://localhost:11434
    model: llama3.2:1b
  provider: GEMINI
  models:
    interpretation: gemini-2.5-flash
    generation_default: gemini-2.5-flash
    generation_quality: gemini-2.5-pro
    embeddings: text-embedding-004
  timeouts:
    interpretation: 1500  # 1.5s for fast query classification
    generation: 3000     # 3s for answer generation
    retrieval: 150       # 150ms for vector retrieval
  rag:
    enabled: true
    chunk_size: 700
    chunk_overlap: 100
    top_k_chunks: 8
  semantic:
    enabled: true
  hybrid:
    enabled: true
    semantic_weight: 0.7
    quality_weight: 0.3
  streaming:
    enabled: false  # Enable later when UI supports it

search:
  lexicon:
    public-sector-tokens: ["kommune","etat","nav","skatt","stat","offentlig","departement","direktorat","helsedirektoratet","kartverket","politiet","helse","utdanningsdirektoratet"]
    customer-synonyms:
      sparebank1: ["sparebank 1","sb1"]
      dnb: ["den norske bank","d.n.b"]
      nav: ["arbeids- og velferdsetaten"]
  ollama:
    base-url: http://localhost:11434
    model: llama3.2:1b
    connect-timeout-seconds: 5
    read-timeout-seconds: 120
    write-timeout-seconds: 10

projectrequest:
  analysis:
    ai-enabled: true
    provider: GEMINI

sync:
  consultants:
    on-startup: true

matching:
  provider: GEMINI
  topN: 10
  enabled: true
  # Model is now configured in gemini.matchingModel above
  # This setting is kept for backwards compatibility but gemini.matchingModel takes precedence
  model: gemini-2.5-pro  # Stable workhorse for batch operations

management:
  endpoints:
    web:
      exposure:
        include: mappings,health
  endpoint:
    health:
      probes:
        enabled: true
      show-details: always
  health:
    # Make health check less strict
    group:
      readiness:
        include: "readinessState,dependencies"

logging:
  level:
    no.cloudberries.candidatematch: INFO # Setter DEBUG-logging for din kode
